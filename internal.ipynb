{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031ddd77",
   "metadata": {},
   "source": [
    "# Internal Notebook for `TLMResponses` Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77feb11a",
   "metadata": {},
   "source": [
    "To allow TLMResponses to parse responses that use web search or file search, we modify the `_format_tools_prompt` function to convert OpenAI Responses built-in tools into function tools, just like the ones users can insert. This way, the agent has context for when the agent uses these tools. These tools are formatted into the system message the same way other user-created functions have been created. The code for this is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790b321",
   "metadata": {},
   "source": [
    "```py\n",
    "elif tool[\"type\"] == \"file_search\":\n",
    "    tool_dict = {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"file_search\",\n",
    "        \"description\": \"Search user-uploaded documents for relevant passages.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"queries\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"description\": \"Search queries to run against the document index.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"queries\"],\n",
    "        },\n",
    "    }\n",
    "elif tool[\"type\"] == \"web_search_preview\":\n",
    "    tool_dict = {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"web_search_call\",\n",
    "        \"description\": \"Search the web for relevant information.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search the web with a query and return relevant pages.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927cca0",
   "metadata": {},
   "source": [
    "Now that the TLM has an understanding of the OpenAI tools it has available to it, we simply have to handle the conversion of the tool calls in the response into expected function calls and responses that are ready for TLM scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45817f3",
   "metadata": {},
   "source": [
    "## File Search\n",
    "\n",
    "For file search, we first need to check if the message can be used for TLM scoring using fetched content. We can do this by checking `message[\"results\"]` which is populated when the user correctly has `include=['file_search_call.results']` in the OpenAI Responses request and unpopulated otherwise. If it's empty, we send out a warning and skip TLM scoring for the specific file search call (the rest of the message chain gets scored still using the normal process). If it has content, however, we can begin processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1904e",
   "metadata": {},
   "source": [
    "```py\n",
    "elif message[\"type\"] == \"file_search_call\":\n",
    "    if message[\"results\"] == None:\n",
    "        warnings.warn(\n",
    "            f\"File search call returned no results. Please include include=['file_search_call.results'] in your request.\",\n",
    "            UserWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        continue\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdf6de",
   "metadata": {},
   "source": [
    "First, we set up a tool call object so the TLM can see that the agent tries to make a call to the file search tool. This will look like a user-generated tool call, but in reality, it is an OpenAI-specific one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f431c",
   "metadata": {},
   "source": [
    "```py\n",
    "tool_call = {\n",
    "    \"name\": \"file_search\",\n",
    "    \"arguments\": {\"queries\": message[\"queries\"]},\n",
    "    \"call_id\": message[\"id\"],\n",
    "}\n",
    "\n",
    "if i == 0 or _get_role(messages[i - 1]) != _ASSISTANT_ROLE:\n",
    "    content_parts.append(_ASSISTANT_PREFIX)\n",
    "content_parts.append(\n",
    "    f\"{_TOOL_CALL_TAG_START}\\n{json.dumps(tool_call, indent=2)}\\n{_TOOL_CALL_TAG_END}\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eab931",
   "metadata": {},
   "source": [
    "We can then show the TLM a tool call response object so that it can see the result of the file search tool call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8e6d2",
   "metadata": {},
   "source": [
    "```py\n",
    "results_list = [\n",
    "    {\n",
    "        \"attributes\": result[\"attributes\"],\n",
    "        \"file_id\": result[\"file_id\"],\n",
    "        \"filename\": result[\"filename\"],\n",
    "        \"score\": result[\"score\"],\n",
    "        \"text\": result[\"text\"],\n",
    "    }\n",
    "    for result in message[\"results\"]\n",
    "]\n",
    "\n",
    "tool_call_response = {\n",
    "    \"name\": \"file_search\",\n",
    "    \"call_id\": message[\"id\"],\n",
    "    \"output\": results_list,\n",
    "}\n",
    "\n",
    "content_parts.append(\"\")\n",
    "content_parts.append(_TOOL_PREFIX)\n",
    "content_parts.append(\n",
    "    f\"{_TOOL_RESPONSE_TAG_START}\\n{json.dumps(tool_call_response, indent=2)}\\n{_TOOL_RESPONSE_TAG_END}\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5181f9e8",
   "metadata": {},
   "source": [
    "The next message that gets processed is the assistant message, so it will function as normal, now with the additional context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b91b2",
   "metadata": {},
   "source": [
    "## Web Search\n",
    "\n",
    "Web search works in a very similar way to file search, only with a different means of fetching data. Because of this similarity, I will only show the real differences between the two.\n",
    "\n",
    "To fetch data, we first need a list of the URLs to search. This can be done by extracting URLs from the assistant message's annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903eaca",
   "metadata": {},
   "source": [
    "```py\n",
    "output_message = [\n",
    "    m[\"content\"][0] for m in messages[i + 1 :] if m[\"type\"] == \"message\"\n",
    "][0]\n",
    "\n",
    "if output_message[\"type\"] == \"refusal\":\n",
    "    continue\n",
    "\n",
    "annotations = list(\n",
    "    set(\n",
    "        [\n",
    "            (annotation[\"url\"], annotation[\"title\"])\n",
    "            for annotation in output_message[\"annotations\"]\n",
    "            if annotation[\"type\"] == \"url_citation\"\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e8e40",
   "metadata": {},
   "source": [
    "We can then use `trafilatura` to extract the main content from the fetched web pages. We also cache these entries because website data may not be stored by the user in message history and can be expensive to re-fetch. The requests for fetching different URLs are done in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c2016",
   "metadata": {},
   "source": [
    "```py\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    def extract_text(pair):\n",
    "        url = pair[0]\n",
    "        if url in _url_cache:\n",
    "            return _url_cache[url]\n",
    "        response = extract(fetch_url(url), output_format=\"markdown\")\n",
    "        _url_cache[url] = response\n",
    "        return response\n",
    "\n",
    "    requests = list(\n",
    "        executor.map(\n",
    "            extract_text,\n",
    "            annotations,\n",
    "        )\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9ad3b",
   "metadata": {},
   "source": [
    "We can then put this into a tool call response like before:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0538b80",
   "metadata": {},
   "source": [
    "```py\n",
    "websites = [\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"content\": data,\n",
    "    }\n",
    "    for (url, title), data in zip(annotations, requests)\n",
    "]\n",
    "\n",
    "content_parts.append(_TOOL_PREFIX)\n",
    "content_parts.append(\n",
    "    f\"{_TOOL_RESPONSE_TAG_START}\\n{json.dumps(tool_response, indent=2)}\\n{_TOOL_RESPONSE_TAG_END}\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e31f9d",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Both of these methods have been tested and have proper responses. Additionally, the code for handling message has been refactored to use the same handling function for prompt and response. This is to make future modifications easier to implement. However, this has had the side-effect of making messages go on another line from the role.\n",
    "\n",
    "Eg.\n",
    "```\n",
    "User:\n",
    "Message\n",
    "```\n",
    "Instead of the previous\n",
    "```\n",
    "User: Message\n",
    "```\n",
    "\n",
    "I don't think this is a major issue, makes message formatting more consistent, and reduces complexity without any real impact on accuracy, so I think this change should be kept. However, some tests in `utils/chat.py` may need to be modified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
